{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 2: SUPPORT VECTOR MACHINES\n",
    "\n",
    "Course 2023/24: *M. Caligiuri*, *P. Talli*, *F. Lincetto*, *F. Chiariotti*, *P. Zanuttigh*\n",
    "\n",
    "The notebook contains some simple tasks to be performed with **SUPPORT VECTOR MACHINES (SVM)**.\n",
    "\n",
    "Complete all the **required code sections** and **answer to all the questions**.\n",
    "\n",
    "### IMPORTANT for the evaluation score:\n",
    "\n",
    "1. **Read carefully all cells** and **follow the instructions**.\n",
    "2. **Re-run all the code from the beginning** to obtain the results for the final version of your notebook, since this is the way we will do it before evaluating your notebooks.\n",
    "3. Make sure to fill the code in the appropriate places **without modifying the template**, otherwise you risk breaking later cells.\n",
    "4. Please **submit the jupyter notebook file (.ipynb)**, do not submit python scripts (.py) or plain text files. **Make sure that it runs fine with the restat&run all command**.\n",
    "5. **Answer the questions in the appropriate cells**, not in the ones where the question is presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Classification with Support Vector Machines\n",
    "\n",
    "In this notebook we are going to explore the use of Support Vector Machines (SVM) for weather classification. We will use a dataset collected using the Luxottica **iSee** glasses. These devices provide multiple **sensors mounted inside the glasses**, which can be accessed through a bluetooth connection.\n",
    "\n",
    "![I-SEE Glasses](data/isee.png \"I-SEE\")\n",
    "\n",
    "The dataset corresponds to 8 hours of atmospherical data recordings sampled every 3 seconds.\n",
    "\n",
    "The dataset labels are the following:\n",
    "\n",
    "| ID  | Label       |\n",
    "| :-: | :-:         |\n",
    "| 0   | Sunny       |\n",
    "| 1   | Rain        |\n",
    "| 2   | Cloudy      |\n",
    "| 3   | Mostly Clear|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary step\n",
    "\n",
    "Place your **name** and **ID number** (matricola) in the cell below. <br>\n",
    "Also recall to **save the file as Surname_Name_LAB02.ipynb**, failure to do so will incur in a **lower grade**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name**: Mario Rossi\n",
    "\n",
    "**ID Number**: 1234567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the heplper functions\n",
    "\n",
    "In this section you will find some helper functions (some already implemented, some to be implemented by you) that will be used in the following sections.\n",
    "1. `load_dataset` -> to load the dataset from the file `data/lux.npz`,\n",
    "2. `plot_input` -> to plot the input data,\n",
    "3. `k_split` ->  to split the trainig dataset in k different folds,\n",
    "4. `k_fold_cross_validation` -> to perform the k-fold cross validation.\n",
    "\n",
    "**DO NOT CHANGE THE PRE-WRITTEN CODE UNLESS OTHERWISE SPECIFIED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load the dataset\n",
    "def load_dataset(path: str) -> (np.ndarray, np.ndarray):\n",
    "    with np.load(path) as data:\n",
    "        x, y = data[\"x\"], data[\"y\"]\n",
    "        \n",
    "        # Normalize the data\n",
    "        x -= x.mean(axis=0)\n",
    "        x /= x.std(axis=0)\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting a image and printing the corresponding label\n",
    "def plot_input(X_matrix: np.ndarray, labels: np.ndarray) -> None:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    cmap = plt.cm.get_cmap('Accent', 4)\n",
    "    im = ax.scatter(X_matrix[:,0], X_matrix[:,1], X_matrix[:,2], c=labels, cmap=cmap)\n",
    "    im.set_clim(-0.5, 3.5)\n",
    "    cbar=fig.colorbar(im, ticks=[0,1,2,3], orientation='vertical', cmap=cmap)\n",
    "    cbar.ax.set_yticklabels(['Sunny', 'Rainy','Cloudy', 'Mostly clear']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset in k different folds\n",
    "def k_split(x: np.ndarray, y:np.ndarray, k: int, shuffle: bool = True) -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
    "    # Shuffle the dataset\n",
    "    if shuffle:\n",
    "        # Create a list of indices\n",
    "        idx = np.arange(x.shape[0])\n",
    "        # Randomly shuffle the indices\n",
    "        np.random.shuffle(idx)\n",
    "        # Shuffle the dataset\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    # Split the dataset in k folds\n",
    "    # ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform the k-fold cross validation\n",
    "def k_fold_cross_validation(x_train: np.ndarray, y_train: np.ndarray, k: int, model: SVC, parameters: dict) -> tuple[tuple, tuple]:\n",
    "    # Define the folds for the cross validation\n",
    "    x_folds, y_folds = k_split(x_train, y_train, k)\n",
    "\n",
    "    # Build a list containing all of the possible combinatioon of parameters\n",
    "    params = list(itertools.product(*parameters.values()))\n",
    "\n",
    "    # Initialize the dictionary of results\n",
    "    results = {k: 0 for k in params}\n",
    "\n",
    "    # For each param combination, perform the SVM training and testing\n",
    "    for param in params:\n",
    "        param = dict(zip(parameters.keys(), param))\n",
    "\n",
    "        fold_accuracies = []\n",
    "        \n",
    "        # ADD YOUR CODE HERE\n",
    "\n",
    "        # Compute the mean accuracy\n",
    "        results[tuple(param.values())] = round(np.mean(fold_accuracies), 4)\n",
    "    \n",
    "    # Find the best parameters\n",
    "    best_parameters = dict(zip(parameters.keys(), params[np.argmax(list(results.values()))]))\n",
    "    best_accuracy = np.max(list(results.values()))\n",
    "    best = (best_parameters, best_accuracy)\n",
    "\n",
    "    # Add the param name to the results\n",
    "    results = [({k: v for k, v in zip(parameters.keys(), p)}, a) for p, a in results.items()]\n",
    "\n",
    "    return best, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Hyper-parameter search\n",
    "\n",
    "### TO DO (A.0)\n",
    "\n",
    "**Set** the random **seed** using your **ID**. If you need to change it for testing add a constant explicitly, eg.: 1234567 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix your ID (\"numero di matricola\") and the seed for random generator\n",
    "# as usual you can try different seeds by adding a constant to the number:\n",
    "# ID = 1234567 + X\n",
    "ID = # YOUR ID\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceding to the training steps, we **load the dataset and split it** in training and test set (while the **training** set is **typically larger**, here we set the number of training samples to 1000 and 4000 for the test data).\n",
    "The **split** is **performed after applying a random permutation** to the dataset, such permutation will **depend on the seed** you set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using the helper function\n",
    "X, y = load_dataset(\"data/lux.npz\")\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The task is quite easy, let's add noise to make it more challenging!\n",
    "# You can try without noise (comment the next 2 lines, easy task), with the suggested amount of noise,\n",
    "# or play with the suggested amount of noise \n",
    "\n",
    "noise = np.random.normal(0, 0.1, X.shape)\n",
    "X = X + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.1)\n",
    "\n",
    "**Divide** the **data into training and test set** (for this part use 1000 samples in the **first** set, 4000 in the **second** one). Make sure that each label is present at least 10 times in training. If it is not, then keep adding permutations to the initial data until this happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random permute the data and split into training and test taking the first 1000\n",
    "# data samples as training and 4000 samples as test\n",
    "permutation = None # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "X = None # ADD YOUR CODE HERE (replace None)\n",
    "y = None # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "m_training = 1000\n",
    "m_test = 4000\n",
    "\n",
    "X_train = None # ADD YOUR CODE HERE (replace None)\n",
    "X_test = None # ADD YOUR CODE HERE (replace None)\n",
    "y_train = None # ADD YOUR CODE HERE (replace None)\n",
    "y_test = None # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape,\"X_test shape:\", X_test.shape,\"||\",\"y_train shape:\",  y_train.shape,\"y_test shape:\", y_test.shape)\n",
    "\n",
    "labels, freqs = None # ADD YOUR CODE HERE. Hint: use np.unique() (replace None)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try the plotting function\n",
    "plot_input(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.2)\n",
    "\n",
    "Use a SVM classfier with cross validation to pick a model. Use a 4-fold cross-validation. Let's start with a Linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for linear SVM\n",
    "parameters = {'C': [ 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# Define the model (without parameters)\n",
    "svm = None # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = None # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "print ('RESULTS FOR LINEAR KERNEL')\n",
    "\n",
    "print(\"Best parameter set found:\")\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "print(\"Score with best parameter:\")\n",
    "# ADD YOUR CODE HERE\n",
    "print()\n",
    "print(\"All scores on the grid:\")\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.3)\n",
    "\n",
    "Pick a model for the Polynomial kernel with degree=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for linear SVM\n",
    "parameters = {'C': [0.01, 0.1, 1],'gamma':[0.01,0.1,1.]}\n",
    "\n",
    "# Define an SVM with poly of degree 2 kernel (without parameters)\n",
    "poly2_svm = None # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = None # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "print ('RESULTS FOR POLY DEGREE=2 KERNEL')\n",
    "\n",
    "print(\"Best parameter set found:\")\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "print(\"Score with best parameter:\")\n",
    "# ADD YOUR CODE HERE\n",
    "print()\n",
    "print(\"All scores on the grid:\")\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.4)\n",
    "\n",
    "Now let's try a higher degree for the polynomial kernel (e.g., 3rd degree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for poly with higher degree kernel\n",
    "parameters = {'C': [0.01, 0.1, 1],'gamma':[0.01,0.1, 1]}\n",
    "\n",
    "# Define an SVM with poly of higher degree kernel (without parameters)\n",
    "degree = 3\n",
    "poly_svm = None # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = None # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "print (f\"RESULTS FOR POLY DEGREE={degree} KERNEL\")\n",
    "\n",
    "print(\"Best parameter set found:\")\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "print(\"Score with best parameter:\")\n",
    "# ADD YOUR CODE HERE\n",
    "print()\n",
    "print(\"All scores on the grid:\")\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.5)\n",
    "\n",
    "Pick a model for the Radial Basis Function kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for rbf SVM\n",
    "parameters = {'C': [0.1, 1, 10, 100],'gamma':[0.001, 0.01, 0.1,1]}\n",
    "\n",
    "# Define an SVM with rbf kernel (without parameters)\n",
    "rbf_svm = None # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "# Perform the K-fold cross validation\n",
    "best, results = None # ADD YOUR CODE HERE (replace None)\n",
    "\n",
    "print ('RESULTS FOR rbf KERNEL')\n",
    "\n",
    "print(\"Best parameter set found:\")\n",
    "# ADD YOUR CODE HERE\n",
    "\n",
    "print(\"Score with best parameter:\")\n",
    "# ADD YOUR CODE HERE\n",
    "print()\n",
    "print(\"All scores on the grid:\")\n",
    "# ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.Q1) [Answer the following]\n",
    "\n",
    "What do you observe when using RBF and polynomial kernels on this dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER A.Q1:**: Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.6)\n",
    "Report here the best SVM kernel and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test error for the best SVM model from CV\n",
    "best_svm = None # USE YOUR OPTIMAL PARAMETERS HERE (replace None)\n",
    "\n",
    "# Run the svm model on the whole training set\n",
    "\n",
    "# Compute the errors\n",
    "# (error is 1 - svm.score)\n",
    "training_error = None # ADD YOUR CODE (replace None)\n",
    "test_error = None # ADD YOUR CODE (replace None)\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (A.7)\n",
    "\n",
    "Analyze how the gamma parameter (inversely proportional to standard deviation of Gaussian Kernel) impact the performances of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different values of gamma\n",
    "# use rbf kernel and C=1\n",
    "\n",
    "# Set gamma values\n",
    "gamma_values = np.logspace(-5,2,8)\n",
    "print(gamma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list, test_acc_list = [], []\n",
    "\n",
    "# ADD YOUR CODE TO TRAIN THE SVM MULTIPLE TIMES WITH THE DIFFERENT VALUES OF GAMMA\n",
    "# PLACE THE TRAIN AND TEST ACCURACY FOR EACH TEST IN THE TRAIN AND TEST ACCURACY LISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "ax[0].plot(gamma_values, train_acc_list)\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_xlabel('gamma')\n",
    "ax[0].set_ylabel('Train accuracy')\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(gamma_values, test_acc_list)\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_xlabel('gamma')\n",
    "ax[1].set_ylabel('Test accuracy')\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) More data\n",
    "Now let's do the same but using more data points for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (B.0)\n",
    "\n",
    "Choose a higher number of data points (e.g. x = 10000) for training data depending on your computing capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = None # ADD YOUR CODE: adjust depending on the capabilities of your PC (replace None)\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "labels, freqs = None # ADD YOUR CODE (replace None)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)\n",
    "\n",
    "# initialize support variables for boundaries visualization\n",
    "granularity = 25\n",
    "x_max = np.abs(X).max()\n",
    "x_range = np.linspace(-x_max, x_max, granularity)\n",
    "x_grid = np.stack(np.meshgrid(x_range, x_range, x_range)).reshape(3, -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (B.1)\n",
    "\n",
    "Let's try to use SVM with parameters obtained from the best model for $m_{training} =  10000$. Since it may take a long time to run, you can decide to just let it run for some time and stop it if it does not complete. If you decide to do this, report it in the TO DO (C.Q1) cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get training and test error for the best SVM model from CV\n",
    "\n",
    "# ADD YOUR CODE\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Boundaries Visualization\n",
    "\n",
    "Now let us plot the classification boundaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.0)\n",
    "\n",
    "Use the SVM to predict on the test set X_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svm_test = None # ADD YOUR CODE (replace None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We constructed a grid of all possible combinations of input values, we now use it to extract the classification boundaries of the three classifiers by having them predict on each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_SVM_grid = rbf.svm.predict(x_grid)\n",
    "\n",
    "rbf_SVM_m = y_test == rbf_svm_test\n",
    "\n",
    "fig = plt.figure(figsize=(20,36))\n",
    "ax1 = fig.add_subplot(1, 3, 1, projection=\"3d\")\n",
    "ax2 = fig.add_subplot(1, 3, 2, projection=\"3d\")\n",
    "ax3 = fig.add_subplot(1, 3, 3, projection=\"3d\")\n",
    "\n",
    "ax1.scatter(x_grid[:,0], x_grid[:,1], x_grid[:,2], c=rbf_SVM_grid, linewidth=0, marker=\"s\", alpha=.05,cmap='Accent')\n",
    "\n",
    "ax1.scatter(X_test[rbf_SVM_m,0], X_test[rbf_SVM_m,1], X_test[rbf_SVM_m,2], c=y_test[rbf_SVM_m], linewidth=.5, edgecolor=\"k\", marker=\".\",cmap='Accent')\n",
    "ax1.scatter(X_test[~rbf_SVM_m,0], X_test[~rbf_SVM_m,1], X_test[~rbf_SVM_m,2], c=y_test[~rbf_SVM_m], linewidth=1, edgecolor=\"r\", marker=\".\",cmap='Accent')\n",
    "ax1.set_xlim([-x_max, x_max])\n",
    "ax1.set_ylim([-x_max, x_max])\n",
    "ax1.set_zlim([-x_max, x_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.Q1) [Answer the following]**\n",
    "\n",
    "Compare and discuss the results from SVM with m=600 and with m=10000 (or whatever value you set) training data points. If you stopped the SVM, include such aspect in your comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER C.Q1:** Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.1)\n",
    "\n",
    "Plot the confusion matrix for the SVM classifier. The confusion matrix has one column for each predicted label and one row for each true label. \n",
    "It shows for each class in the corresponding row how many samples belonging to that class gets each possible output label. Notice that the diagonal contains the correctly classified samples, while the other cells correspond to errors. You can obtain it with the sklearn.metrics.confusion_matrix function (see the documentation). You can also print also the normalized confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True) # for better aligned printing of confusion matrix use floatmode='fixed'\n",
    "\n",
    "u, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Labels and frequencies in test set: \", counts)\n",
    "\n",
    "confusion_SVM =  # ADD YOUR CODE\n",
    "print(\"\\n Confusion matrix SVM  \\n \\n\", confusion_SVM)\n",
    "print(\"\\n Confusion matrix SVM (normalized)   \\n \\n\", confusion_SVM /counts[:,None] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "    \n",
    "im = plt.imshow(confusion_SVM /counts[:,None], cmap=\"Blues\",interpolation='nearest')\n",
    "plt.xticks([0,1,2,3], ['Sunny', 'Rainy','Cloudy', 'Mostly clear'],ha=\"right\",rotation=30)\n",
    "plt.yticks([0,1,2,3], ['Sunny', 'Rainy','Cloudy', 'Mostly clear'],ha=\"right\",rotation=30)\n",
    "cm = confusion_SVM /counts[:,None]\n",
    "fmt = '.2f'\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "        ha=\"center\", va=\"center\",\n",
    "        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.colorbar(im, location='bottom')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO (C.Q2) [Answer the following]\n",
    "\n",
    "Have a look at the confusion matrix and comment on the obtained accuracies. Why some classes have lower accuracies and others an higher one? Make some guesses on the possible causes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER C.Q2:** Answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
